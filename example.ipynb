{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical models for aggregating LLM or human coders\n",
    "\n",
    "This notebook is a simple example of how to use statistical models to aggregate labels generated by LLMs or human coders. Each coder generates a label for each item, and each item is labeled by multiple coders. The goal is to aggregate the labels to get a single label for each item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoagg import DSModel, MACEModel, BACEModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MACE (Multi-Annotator Competence Estimation; Hovy et al., 2013)\n",
    "\n",
    "$z_i \\in \\{1, ..., K\\}$ is the true label. The base rates of each\n",
    "category are also unknown and stored in a vector $\\alpha \\in\n",
    "\\Delta_K$.\n",
    "\n",
    "I have $J$ coders, and each gives me a coding for each item $i$, $x_{ij}$.\n",
    "Each coder has competence $\\beta_j$. Before a coder generates a label,\n",
    "nature flips a coin and decides to return a true label with $\\beta_j$\n",
    "probability, and a bad label with complementary probability.\n",
    "\n",
    "\n",
    "+ $z_i \\sim \\text{Categorical}(\\alpha)$ : True (hidden) labels\n",
    "+ $\\beta_j$ : Coder j's competence parameter\n",
    "+ $c_{ij} \\sim \\text{Bernoulli}(\\beta_j)$ : Coder j's annotation\n",
    "+ $g_{ij} \\sim \\text{Uniform(1, K)}$ : Coder j's guess\n",
    "+ $x_{ij} = c_{ij} z_i + (1-c_{ij}) g_{ij}$ : Observed labels from coder j for item i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By Bayes Rule, we can solve for the true label distribution given the\n",
    "observed labels.\n",
    "\n",
    "\\begin{align*}\n",
    "P(z_i = k | x_{ij} = k) &= \\frac{ P(x_{ij} = k | z_i = k) \\cdot\n",
    "P(z_i = k)}{P(x_{ij} = k)} \\\\\n",
    "& =\n",
    "\\frac{ P(x_{ij} = k | z_i = k) \\cdot\n",
    "P(z_i = k)}\n",
    "{\n",
    "\\underbrace{P(x_{ij} = k | z_i = k)}_{[\\beta_j + (1-\\beta_j)/K]}\n",
    "\\underbrace{P(z_i = k)}_{\\alpha_k} +\n",
    "\\underbrace{P(x_{ij} = k | z_i \\neq k)}_{(1-\\beta_j)/K}\n",
    "\\underbrace{P(z_i \\neq k)}_{1-\\alpha_k}\n",
    "} \\\\\n",
    "& =\n",
    "\\frac{[\n",
    "  \\beta_j + (1-\\beta_j)/K]\\alpha_k}{\n",
    "[\\beta_j + (1-\\beta_j)/K]\\alpha_k +\n",
    "(1-\\beta_j)/K(1-\\alpha_k)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACE (Biased Annotator Competence Estimation; Tyler 2021)\n",
    "\n",
    "Each coder may have different 'biases', which means that they don't guess uniformly. Furthermore, it may actually be the case that the base rates for each category are not uniform. To accommodate this, we alter the model as follows:\n",
    "\n",
    "+ $z_i \\sim \\text{Categorical}(\\alpha)$ : True (hidden) labels\n",
    "+ $\\beta_j$ : Coder j's competence parameter\n",
    "+ $c_{ij} \\sim \\text{Bernoulli}(\\beta_j)$ : Coder j's annotation\n",
    "+ $g_{ij} \\sim \\text{Categorical}(\\gamma_j)$ : Coder j's guess drawn from a distribution. When $\\gamma_j = \\frac{1}{K} \\mathbf{1}$, this is the same as MACE.\n",
    "+ $x_{ij} = c_{ij} z_i + (1-c_{ij}) g_{ij}$ : Observed labels from coder j for item i\n",
    "\n",
    "Under BACE, \n",
    "\n",
    "$$\n",
    "P(x_{ij} = k | z_i = k) = \\beta_j + (1 - \\beta_j) \\gamma_{jk}\n",
    "$$\n",
    "\n",
    "Accordingly, the label distribution given observed labels is \n",
    "\n",
    "\\begin{align*}\n",
    "P(z_i = k | x_{ij} = k) &= \\frac{ P(x_{ij} = k | z_i = k) \\cdot\n",
    "P(z_i = k)}{P(x_{ij} = k)} \\\\\n",
    "& =\n",
    "\\frac{ P(x_{ij} = k | z_i = k) \\cdot\n",
    "P(z_i = k)}\n",
    "{\n",
    "P(x_{ij} = k | z_i = k) P(z_i = k) +\n",
    "P(x_{ij} = k | z_i \\neq k) P(z_i \\neq k)\n",
    "} \\\\\n",
    "& =\n",
    "\\frac{[\n",
    "  \\beta_j + (1-\\beta_j)\\gamma_{jk}]\\alpha_k}{\n",
    "[\\beta_j + (1-\\beta_j)\\gamma_{jk}]\\alpha_k +\n",
    "(1-\\beta_j)\\gamma_{jk}(1-\\alpha_k)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(\n",
    "    n_items=100, n_coders=5, n_classes=5, coder_reliabilities=None, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with two groups of items having different coder behaviors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_items : int\n",
    "        Number of items to generate\n",
    "    n_coders : int\n",
    "        Number of coders\n",
    "    n_classes : int\n",
    "        Number of possible classes/categories\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        Generated data with columns [ii, jj, yy, group]\n",
    "    true_labels : numpy array\n",
    "        Ground truth labels for items\n",
    "    group_ids : numpy array\n",
    "        Group assignments for items\n",
    "    coder_reliabilities : numpy array\n",
    "        Reliability scores for each coder\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Generate true labels\n",
    "    true_labels = np.random.randint(0, n_classes, n_items)\n",
    "\n",
    "    # Assign items to groups A and B\n",
    "    group_ids = np.random.choice([\"A\", \"B\"], n_items)\n",
    "\n",
    "    # Create coder reliabilities if not provided\n",
    "    if coder_reliabilities is None:\n",
    "        coder_reliabilities = np.random.uniform(0.4, 0.9, n_coders)\n",
    "\n",
    "    # Generate coder-specific guessing distributions\n",
    "    middle_category = (n_classes - 1) / 2  # Allow for even number of classes\n",
    "    group_A_guess_dist = np.zeros((n_coders, n_classes))\n",
    "    group_B_guess_dist = np.ones((n_coders, n_classes)) / n_classes\n",
    "\n",
    "    # Create gaussian-like distribution for group A centered at middle category\n",
    "    x = np.arange(n_classes)\n",
    "    for j in range(n_coders):\n",
    "        # Group A: Peak at middle category with gaussian-like distribution\n",
    "        dist = np.exp(-0.5 * ((x - middle_category) / (n_classes / 5)) ** 2)\n",
    "        group_A_guess_dist[j] = dist / dist.sum()\n",
    "\n",
    "    # Generate labels\n",
    "    data = []\n",
    "    for i in range(n_items):\n",
    "        for j in range(n_coders):\n",
    "            # Determine if coder is correct\n",
    "            is_correct = np.random.random() < coder_reliabilities[j]\n",
    "\n",
    "            if is_correct:\n",
    "                label = true_labels[i]\n",
    "            else:\n",
    "                # Choose guessing distribution based on group\n",
    "                if group_ids[i] == \"A\":\n",
    "                    label = np.random.choice(n_classes, p=group_A_guess_dist[j])\n",
    "                else:\n",
    "                    label = np.random.choice(n_classes, p=group_B_guess_dist[j])\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"ii\": i + 1,  # 1-based indexing\n",
    "                    \"jj\": j + 1,\n",
    "                    \"yy\": label + 1,\n",
    "                    \"group\": group_ids[i],\n",
    "                    \"true_label\": true_labels[i] + 1,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # add a column for the true labels\n",
    "    return df, true_labels, group_ids, coder_reliabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(df, true_labels, group_ids, printres=False):\n",
    "    \"\"\"\n",
    "    Fit and evaluate all three models\n",
    "    \"\"\"\n",
    "    n_classes = len(np.unique(df[\"yy\"]))\n",
    "    n_coders = len(np.unique(df[\"jj\"]))\n",
    "\n",
    "    # Initialize models\n",
    "    mace = MACEModel(n_classes, n_coders)\n",
    "    bace = BACEModel(n_classes, n_coders)\n",
    "    ds = DSModel(n_classes, n_coders)\n",
    "\n",
    "    models = {\"MACE\": mace, \"BACE\": bace, \"DS\": ds}\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Fit model\n",
    "        for _ in range(50):  # EM iterations\n",
    "            model.map_update(df)\n",
    "\n",
    "        # Get predictions\n",
    "        final_results = model.calc_logliks(df)\n",
    "        predictions = np.argmax(final_results[\"post_Z\"], axis=1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        # Calculate accuracy per group\n",
    "        group_A_mask = group_ids == \"A\"\n",
    "        group_B_mask = group_ids == \"B\"\n",
    "        acc_A = accuracy_score(true_labels[group_A_mask], predictions[group_A_mask])\n",
    "        acc_B = accuracy_score(true_labels[group_B_mask], predictions[group_B_mask])\n",
    "\n",
    "        results[name] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"accuracy_A\": acc_A,\n",
    "            \"accuracy_B\": acc_B,\n",
    "            \"predictions\": predictions,\n",
    "            \"model\": model,\n",
    "        }\n",
    "\n",
    "    if printres:\n",
    "        maj_voting = pd.concat(\n",
    "            [\n",
    "                df.groupby(\"ii\")[\"yy\"].apply(lambda x: pd.Series.mode(x)[0]),\n",
    "                df.groupby(\"ii\")[\"true_label\"].first(),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        maj_voting.columns = [\"pred\", \"tru\"]\n",
    "        print(\n",
    "            \"Majority voting accuracy: \",\n",
    "            accuracy_score(maj_voting[\"tru\"], maj_voting[\"pred\"]),\n",
    "        )\n",
    "        # Print results\n",
    "        print(\"\\nOverall Accuracy:\")\n",
    "        for name, res in results.items():\n",
    "            print(f\"{name}: {res['accuracy']:.3f}\")\n",
    "\n",
    "        print(\"\\nGroup A Accuracy (Middle-category bias):\")\n",
    "        for name, res in results.items():\n",
    "            print(f\"{name}: {res['accuracy_A']:.3f}\")\n",
    "\n",
    "        print(\"\\nGroup B Accuracy (Random guessing):\")\n",
    "        for name, res in results.items():\n",
    "            print(f\"{name}: {res['accuracy_B']:.3f}\")\n",
    "\n",
    "        # # Print estimated coder reliabilities\n",
    "        # print(\"\\nEstimated vs True Coder Reliabilities:\")\n",
    "        # print(\"True:\", coder_reliabilities)\n",
    "        # for name, res in results.items():\n",
    "        #     if hasattr(res[\"model\"], \"beta\"):\n",
    "        #         print(f\"{name}:\", res[\"model\"].beta)\n",
    "\n",
    "        # # For BACE, show learned guessing distributions\n",
    "        # if \"BACE\" in results:\n",
    "        #     print(\"\\nBACE Learned Guessing Distributions:\")\n",
    "        #     bace_model = results[\"BACE\"][\"model\"]\n",
    "        #     print(bace_model.gamma)\n",
    "        return None\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ii</th>\n",
       "      <th>jj</th>\n",
       "      <th>yy</th>\n",
       "      <th>group</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ii  jj  yy group  true_label\n",
       "0   1   1   4     B           4\n",
       "1   1   2   4     B           4\n",
       "2   1   3   4     B           4\n",
       "3   1   4   4     B           4\n",
       "4   1   5   4     B           4\n",
       "5   1   6   4     B           4\n",
       "6   2   1   4     A           5\n",
       "7   2   2   5     A           5\n",
       "8   2   3   2     A           5\n",
       "9   2   4   3     A           5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, true_labels, group_ids, coder_reliabilities = generate_synthetic_data(\n",
    "    n_coders=6, n_classes=5\n",
    ")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting accuracy:  0.88\n",
      "\n",
      "Overall Accuracy:\n",
      "MACE: 0.920\n",
      "BACE: 0.930\n",
      "DS: 0.900\n",
      "\n",
      "Group A Accuracy (Middle-category bias):\n",
      "MACE: 0.929\n",
      "BACE: 0.946\n",
      "DS: 0.911\n",
      "\n",
      "Group B Accuracy (Random guessing):\n",
      "MACE: 0.909\n",
      "BACE: 0.909\n",
      "DS: 0.886\n"
     ]
    }
   ],
   "source": [
    "evaluate_models(df, true_labels, group_ids, printres=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting accuracy:  0.91\n",
      "\n",
      "Overall Accuracy:\n",
      "MACE: 0.890\n",
      "BACE: 0.920\n",
      "DS: 0.900\n",
      "\n",
      "Group A Accuracy (Middle-category bias):\n",
      "MACE: 0.893\n",
      "BACE: 0.929\n",
      "DS: 0.893\n",
      "\n",
      "Group B Accuracy (Random guessing):\n",
      "MACE: 0.886\n",
      "BACE: 0.909\n",
      "DS: 0.909\n"
     ]
    }
   ],
   "source": [
    "df, true_labels, group_ids, coder_reliabilities = generate_synthetic_data(\n",
    "    n_coders=3, n_classes=5\n",
    ")\n",
    "evaluate_models(df, true_labels, group_ids, printres=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting accuracy:  0.82\n",
      "\n",
      "Overall Accuracy:\n",
      "MACE: 0.840\n",
      "BACE: 0.860\n",
      "DS: 0.320\n",
      "\n",
      "Group A Accuracy (Middle-category bias):\n",
      "MACE: 0.807\n",
      "BACE: 0.825\n",
      "DS: 0.316\n",
      "\n",
      "Group B Accuracy (Random guessing):\n",
      "MACE: 0.884\n",
      "BACE: 0.907\n",
      "DS: 0.326\n"
     ]
    }
   ],
   "source": [
    "df, true_labels, group_ids, coder_reliabilities = generate_synthetic_data(\n",
    "    n_coders=3, n_classes=20\n",
    ")\n",
    "evaluate_models(df, true_labels, group_ids, printres=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting accuracy:  0.81\n",
      "\n",
      "Overall Accuracy:\n",
      "MACE: 0.880\n",
      "BACE: 0.850\n",
      "DS: 0.840\n",
      "\n",
      "Group A Accuracy (Middle-category bias):\n",
      "MACE: 0.895\n",
      "BACE: 0.860\n",
      "DS: 0.825\n",
      "\n",
      "Group B Accuracy (Random guessing):\n",
      "MACE: 0.860\n",
      "BACE: 0.837\n",
      "DS: 0.860\n"
     ]
    }
   ],
   "source": [
    "df, true_labels, group_ids, coder_reliabilities = generate_synthetic_data(\n",
    "    n_coders=5, n_classes=10\n",
    ")\n",
    "evaluate_models(df, true_labels, group_ids, printres=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(hypothetically)\n",
    "\n",
    "## LLM labels\n",
    "\n",
    "```python\n",
    "temperatures = [0.1, 0.3, 0.7, 1.0, 1.5]\n",
    "models = [\"gpt-4\", \"gpt-3.5\", \"claude\", \"llama\"]\n",
    "\n",
    "# Generate labels\n",
    "labels = []\n",
    "for temp in temperatures:\n",
    "    for model in models:\n",
    "        response = get_model_response(prompt, model=model, temperature=temp)\n",
    "        labels.append({\n",
    "            \"ii\": item_id,\n",
    "            \"jj\": f\"{model}_{temp}\",\n",
    "            \"yy\": response\n",
    "        })\n",
    "\n",
    "# Use BACE to aggregate\n",
    "bace = BACEModel(n_classes=num_classes, n_coders=len(temperatures)*len(models))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
